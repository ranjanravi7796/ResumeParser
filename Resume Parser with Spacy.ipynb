{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resume Parser with Spacy\n",
    "\n",
    "This notebook is used to parse resumes and extract their Universities, companies, skills, total work experience, Programming Experience and Database experience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import os\n",
    "import numpy\n",
    "import math\n",
    "import natsort \n",
    "from pdfminer.high_level import extract_text\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import spacy\n",
    "import pickle\n",
    "import random\n",
    "from spacy.training.example import Example\n",
    "from collections import defaultdict\n",
    "import re\n",
    "from datetime import date\n",
    "import configurations as regex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading extracted contents from resumes \n",
    "test_data = pandas.read_excel('input.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# consider the last 117 resumes as test data\n",
    "test_data = test_data[100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './train'\n",
    "files = os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorting the filenames\n",
    "files = natsort.natsorted(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# considering the last 117 files alone\n",
    "files = files[100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting resume contents \n",
    "resume_text_list = []\n",
    "for f in files:\n",
    "    resume_text = extract_text(path+'/'+f)\n",
    "    resume_text_list.append(resume_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['resume_text'] = resume_text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanResume(resumeText):\n",
    "    \n",
    "    ''' This function is used to clean the resume contents i.e., removing URLS, punctuations, newline and extra whitespaces'''\n",
    "        \n",
    "    resumeText = re.sub('httpS+s*', ' ', resumeText)  # remove URLs\n",
    "    resumeText = re.sub('[%s]' % re.escape(\"\"\"!\"#$%&'()*+,-./:;<=>?@[]^_`{|}~\"\"\"), ' ', resumeText)  # remove punctuations\n",
    "    resumeText = re.sub('\\s+', ' ', resumeText)  # remove extra whitespace\n",
    "    resumeText = re.sub('\\n', ' ', resumeText)  # remove newline\n",
    "    \n",
    "    return resumeText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling the cleanResume function\n",
    "test_data['cleaned_resume'] = test_data.resume_text.apply(lambda x: cleanResume(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking out only the cleaned resume contents\n",
    "cleaned_resume = test_data[['cleaned_resume']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the pre-trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us load the spacy model which was trained on 300 resumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_model = spacy.load('nlp_model_with_100_resumes_tuned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0x1abeb8c72b0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## University Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let us parse the universities from each resume from the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_college_dict = defaultdict(list)\n",
    "index = 100\n",
    "for r in cleaned_resume['cleaned_resume']:\n",
    "    doc = nlp_model(r)  # passing the resume text to the model\n",
    "    index += 1\n",
    "    prediction_college_dict[index] = []  # model might return more than one string and thatswhy we are having list here.\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == 'College Name':\n",
    "            prediction_college_dict[index].append(ent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key,value in prediction_college_dict.items():\n",
    "    prediction_college_dict[key] = list(set(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the actual universities for each resume from the test data for comparing with the model results\n",
    "\n",
    "actual_college_dict = {}\n",
    "index = 100\n",
    "for idx in range(100,217):\n",
    "    index = idx + 1\n",
    "    actual_college_dict[index] = []\n",
    "    if (isinstance(test_data.loc[idx]['UniversityofUSAttendees'], float) and (numpy.isnan(test_data.loc[216]['UniversityofUSAttendees']))):\n",
    "        actual_college_dict[index] = test_data.loc[idx]['UniversityofIndianAttendees']\n",
    "    else:\n",
    "        actual_college_dict[index] = test_data.loc[idx]['UniversityofUSAttendees']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating two lists: actual universities and prediction universities. This step is required for creating a dataframe.\n",
    "actual = []\n",
    "prediction = []\n",
    "for idx in range(101,218):\n",
    "    actual.append(actual_college_dict[idx])\n",
    "    prediction.append(prediction_college_dict[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a college dataframe containing both the actual and predicted universities\n",
    "college_df = pandas.DataFrame(list(zip(actual,prediction)),columns=['Actual College Name','Predicted College Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exporting the college dataframe to excel file\n",
    "college_df.to_excel('College Predictions.xlsx',header=True,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Company parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us parse the companies from each resume from the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_dict = defaultdict(list)\n",
    "index = 100\n",
    "for r in cleaned_resume['cleaned_resume']:\n",
    "    doc = nlp_model(r)    # passing the resume text to the model\n",
    "    index += 1\n",
    "    company_dict[index] = []   # model might return more than one string and thatswhy we are having list here.\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == 'Companies worked at':\n",
    "            company_dict[index].append(ent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_company_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key,value in company_dict.items():\n",
    "    prediction_company_list.append(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a company dataframe containing the predicted companies\n",
    "company_df = pandas.DataFrame(list(zip(prediction_company_list)),columns=['Predicted Companies'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exporting the company dataframe to excel file\n",
    "company_df.to_excel('Company_Predictions.xlsx',header=True,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skills parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us parse the skills from each resume from the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "skills_dict = defaultdict(list)\n",
    "index = 100\n",
    "for r in cleaned_resume['cleaned_resume']:\n",
    "    doc = nlp_model(r)   # passing the resume text to the model\n",
    "    index += 1\n",
    "    skills_dict[index] = []   # model might return more than one string and thatswhy we are having list here\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == 'Skills':\n",
    "            skills_dict[index].append(ent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "skills_list = []\n",
    "for k,v in exp_dict.items():\n",
    "    skills_list.append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a skills dataframe containing the predicted skills\n",
    "skills_df = pandas.DataFrame(list(zip(skills_list)),columns=['Predicted Skills'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exporting the skills dataframe to excel file\n",
    "skills_df.to_excel('Skills_Prediction.xlsx',header=True,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work Experience Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us parse the total years of work experience for each resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of possible section keywords that can be found in a resume\n",
    "experience = ['experience', 'work experience', 'workexperience', 'professional experience', 'professionalexperience',\n",
    "             'industry experience','industryexperience','industrial experience','industrialexperience']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "experience_dict = defaultdict(list)\n",
    "index = 100\n",
    "for res_text in cleaned_resume['cleaned_resume']:  # loop over the resume text\n",
    "    \n",
    "    index += 1\n",
    "    \n",
    "    start_year = -1\n",
    "    end_year = -1\n",
    "    \n",
    "    resume_text = res_text.lower()  # convert the entire resume text to lowercase\n",
    "    exp_text_found = ''\n",
    "    for exp in experience:  # loop over all possible experience keywords\n",
    "        exp_text = resume_text.split(exp)\n",
    "        if len(exp_text) > 1:  # if any one of the experience section keyword found\n",
    "            exp_text_found = exp_text[-1]\n",
    "            \n",
    "    if len(exp_text_found) == 0:  # if there is no experience section keyword found\n",
    "        experience_dict[index] = 0   # assigning experience as 0\n",
    "    \n",
    "    else:\n",
    "        regular_expression = re.compile(regex.date_range, re.IGNORECASE)    # reg expression to check for date range\n",
    "        regex_result = re.search(regular_expression,exp_text_found)\n",
    "        \n",
    "        while regex_result:   # if a date range is found\n",
    "            date_range = regex_result.group()\n",
    "            year_regex = re.compile(regex.year)  # reg expression to check for a year\n",
    "            year_result = re.search(year_regex, date_range)\n",
    "            if (start_year == -1) or (int(year_result.group()) <= start_year):\n",
    "                start_year = int(year_result.group())\n",
    "\n",
    "            if date_range.lower().find('present') != -1:  # if there is a present string in the date range\n",
    "                end_year = date.today().year # current year\n",
    "            else:\n",
    "                year_result = re.search(year_regex, date_range[year_result.end():])\n",
    "                if (end_year == -1) or (int(year_result.group()) >= end_year):\n",
    "                    end_year = int(year_result.group())\n",
    "                \n",
    "            exp_text_found = exp_text_found[regex_result.end():]\n",
    "            regex_result = re.search(regular_expression, exp_text_found)\n",
    "            \n",
    "        experience_dict[index] = end_year - start_year   # total experience = end working year - start working year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_exp = list(experience_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating an experience dataframe containing the total years of work experience\n",
    "experience_df = pandas.DataFrame(pred_exp,columns=['Pred_exp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exporting the experience dataframe to excel file\n",
    "experience_df.to_excel('Work Experience Prediction.xlsx',index=False,header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us build a random forest classifier to predict whether the candidate have programming and database experience or not.\n",
    "For this, let's read the all the resumes and then consider first 100 resumes as train and the remaining 117 resumes as test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading extracted contents from resumes\n",
    "input_data = pandas.read_excel('input.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the resume filenames\n",
    "path = './train'\n",
    "files = os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the resume filenames\n",
    "files = natsort.natsorted(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the resume contents from the resume\n",
    "\n",
    "resume_text_list = []\n",
    "for f in files:\n",
    "    resume_text = extract_text(path+'/'+f)\n",
    "    resume_text_list.append(resume_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append a new resume content column \n",
    "input_data['resume_text'] = resume_text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling the cleanResume function\n",
    "input_data['cleaned_resume'] = input_data.resume_text.apply(lambda x: cleanResume(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programming Experience Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's create a classifier to predict the programming experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data['ProgrammingExperience'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "programming_clf = input_data[['ProgrammingExperience','cleaned_resume']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using LabelEncoder to label Yes to 1 and No to 0\n",
    "var_mod = ['ProgrammingExperience']\n",
    "le = LabelEncoder()\n",
    "for i in var_mod:\n",
    "    programming_clf[i] = le.fit_transform(programming_clf[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all resume text values\n",
    "requiredText = programming_clf['cleaned_resume'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "requiredTarget = programming_clf['ProgrammingExperience'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a TFidfVectorizer matrix out of the given resume text\n",
    "word_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    stop_words='english',\n",
    "    max_features=None)\n",
    "word_vectorizer.fit(requiredText)\n",
    "WordFeatures = word_vectorizer.transform(requiredText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_array = numpy.array(word_vectorizer.get_feature_names())  \n",
    "tfidf_sorting = numpy.argsort(WordFeatures.toarray()).flatten()[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1000\n",
    "top_n = feature_array[tfidf_sorting][:n]  # taking out top 1000 important features/words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# again build a TfIDFVectorizer considering the top 1000 important features as vocabulary\n",
    "word_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    stop_words='english',\n",
    "    vocabulary=top_n)\n",
    "word_vectorizer.fit(requiredText)\n",
    "WordFeatures = word_vectorizer.transform(requiredText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take first 100 resumes as train and the remaining 117 resumes as test\n",
    "X_train = WordFeatures[:100]\n",
    "y_train = requiredTarget[:100]\n",
    "X_test = WordFeatures[100:]\n",
    "y_test = requiredTarget[100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a randomforest classifier for making this prediction\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(X_train, y_train)  # model fit with the train data\n",
    "prediction = clf.predict(X_test)  # predict on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find accuracy for the prediction\n",
    "accuracy_score(y_test,prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database Experience Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us focus on the Database experience and build a classifier to predict whether each resume have a database experience or not. Here, we are using the TfIdfVectorizer built above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data['DatabaseExperience'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_clf = input_data[['DatabaseExperience','cleaned_resume']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now my target is DatabaseExperience\n",
    "requiredTarget = database_clf['DatabaseExperience'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# consider first 100 resumes as train and the remaining 117 resumes as test\n",
    "X_train = WordFeatures[:100]\n",
    "y_train = requiredTarget[:100]\n",
    "X_test = WordFeatures[100:]\n",
    "y_test = requiredTarget[100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a randomforest classifier to predict the database experience\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(X_train, y_train)    # model fit with the train data\n",
    "prediction = clf.predict(X_test)   # predict on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find accuracy for the database experience prediction\n",
    "accuracy_score(y_test,prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
