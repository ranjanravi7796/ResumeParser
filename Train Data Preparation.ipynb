{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Data Preparation\n",
    "\n",
    "This notebook is about SpaCy annotator for Named Entity Recognition (NER) using ipywidgets. In this notebook, we will be preparing a training data set. First, the list of resumes will be read. Then, it would be converted to the Spacy input data format. The annotator allows users to quickly assign (custom) labels to one or more entities in the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy_annotator as spa\n",
    "import pandas\n",
    "import os\n",
    "import numpy\n",
    "import natsort \n",
    "from pdfminer.high_level import extract_text\n",
    "import re\n",
    "import spacy\n",
    "import pickle\n",
    "import random\n",
    "from spacy.training.example import Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading extracted contents from resumes\n",
    "input_data = pandas.read_excel('input.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting the list of resumes which we are gonna convert it into a Spacy input data format. \n",
    "start = 90\n",
    "end = 100\n",
    "input_data = input_data[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the resume filenames\n",
    "path = './train'\n",
    "files = os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the resume filenames\n",
    "files = natsort.natsorted(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = files[90:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['resume91.pdf',\n",
       " 'resume92.pdf',\n",
       " 'resume93.pdf',\n",
       " 'resume94.pdf',\n",
       " 'resume95.pdf',\n",
       " 'resume96.pdf',\n",
       " 'resume97.pdf',\n",
       " 'resume98.pdf',\n",
       " 'resume99.pdf',\n",
       " 'resume100.pdf']"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the resume contents from the resume\n",
    "resume_text_list = []\n",
    "for f in files:\n",
    "    resume_text = extract_text(path+'/'+f)\n",
    "    resume_text_list.append(resume_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append a new resume content column \n",
    "input_data['resume_text'] = resume_text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanResume(resumeText):\n",
    "    ''' This function is used to clean the resume contents i.e., removing URLS, punctuations, newline and extra whitespaces'''\n",
    "    \n",
    "    resumeText = re.sub('httpS+s*', ' ', resumeText)  # remove URLs\n",
    "    resumeText = re.sub('[%s]' % re.escape(\"\"\"!\"#$%&'()*+,-./:;<=>?@[]^_`{|}~\"\"\"), ' ', resumeText)  # remove punctuations\n",
    "    resumeText = re.sub('\\s+', ' ', resumeText)  # remove extra whitespace\n",
    "    resumeText = re.sub('\\n', ' ', resumeText)  # remove newline\n",
    "    \n",
    "    return resumeText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling the cleanResume function\n",
    "input_data['cleaned_resume'] = input_data.resume_text.apply(lambda x: cleanResume(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking out only the cleaned resume contents\n",
    "cleaned_resume = input_data[['cleaned_resume']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_resume.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading a blank Spacy model with en_core_web_sm which is a small English pipeline trained on written web text (For example: blogs, news, comments) that includes vocabulary, syntax and entities.\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating my own list of Entities\n",
    "annotator = spa.Annotator(labels=['College Name',\n",
    " 'Companies worked at',\n",
    " 'Degree',\n",
    " 'Designation',\n",
    " 'Email Address',\n",
    " 'Graduation Year',\n",
    " 'Location',\n",
    " 'Name',\n",
    " 'Skills',\n",
    " 'UNKNOWN',\n",
    " 'Work Start Year',\n",
    " 'Work End Year',\n",
    " 'Years of Experience'], model=nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d84fdd7e5b014e59a9874f5466b5bb27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='-1 examples annotated, 11 examples left')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "627f95333e004ba6b9dd2e9410693479",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='College Name', layout=Layout(width='auto'), placeholder='ent one, ent two, ent thr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9436f3d1a87494a89a929e85267a8c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='Companies worked at', layout=Layout(width='auto'), placeholder='ent one, ent two, …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ebce0e5dc8043ada6a6a06712b61cd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='Degree', layout=Layout(width='auto'), placeholder='ent one, ent two, ent three')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37a34e91d32442aaaeb02c0e2317e1b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='Designation', layout=Layout(width='auto'), placeholder='ent one, ent two, ent thre…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a621f85238b429b827d9bd0c9e88c56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='Email Address', layout=Layout(width='auto'), placeholder='ent one, ent two, ent th…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43c70899f6344dc1acd6739a85f75a1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='Graduation Year', layout=Layout(width='auto'), placeholder='ent one, ent two, ent …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4b94304e3e04c4ebcb6f4e2b7ed4caa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='Location', layout=Layout(width='auto'), placeholder='ent one, ent two, ent three')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "824a9af44b53459b99c9b12c1d349f85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='Name', layout=Layout(width='auto'), placeholder='ent one, ent two, ent three')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44277a1a68504313a76babad06417e74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='Skills', layout=Layout(width='auto'), placeholder='ent one, ent two, ent three')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc1f4f88433d4f1f8a99786c51b5b2eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='UNKNOWN', layout=Layout(width='auto'), placeholder='ent one, ent two, ent three')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dead153e2cc24106bf9398ab34fc98e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='Work Start Year', layout=Layout(width='auto'), placeholder='ent one, ent two, ent …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b46162e26f74312981a3231b39a1698",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='Work End Year', layout=Layout(width='auto'), placeholder='ent one, ent two, ent th…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5878d0af612f4684bde62c2d2075bec6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='Years of Experience', layout=Layout(width='auto'), placeholder='ent one, ent two, …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "badb65d1dd9a4960abef23cd22a19dad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Button(button_style='success', description='submit', style=ButtonStyle()), Button(button_style=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e936b6e2a0d44dba6ea8df34899aa01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# this line would open up the ipywidgets where we can assign each text to its entity by highlighting the text. \n",
    "# The output df_labels would have the resume text with the list of entities of its starting and ending indices.\n",
    "df_labels = annotator.annotate(df=cleaned_resume, col_text=\"cleaned_resume\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_labels['annotations'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the df_labels to a text file so that I can combine them with the original training dataset of 200 resumes.\n",
    "df_labels['annotations'].to_csv('90_to_100_resumes.txt', sep=' ', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is an open source project available on GitHub where they have trained 200 resumes on a Spacy model with the similar approach. Their training data is in the text format containing the resume text with their entities of start and end indices.\n",
    "\n",
    "Now, we are gonna combine our prepared training data of 100 resumes to the original training data of 200 resumes from GitHub.\n",
    "Therefore, in total, we have 300 rows of training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the train data of 200 resumes\n",
    "resume_train_data = pickle.load(open('resume_train_data.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# total 200 resumes data\n",
    "len(resume_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# appending my 100 resumes to the original 200 resumes\n",
    "for d in df_labels['annotations']:\n",
    "    resume_train_data.append(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model building and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading pre-existing blank spacy model\n",
    "nlp = spacy.blank('en')\n",
    "\n",
    "def train_model(resume_train_data):\n",
    "    '''\n",
    "    This function helps to train the spacy model with the new entities\n",
    "    '''\n",
    "            \n",
    "    # adding the ner pipeline component to the blank spacy model\n",
    "    if 'ner' not in nlp.pipe_names:\n",
    "        ner = nlp.add_pipe('ner')\n",
    "        \n",
    "    # adding the user defined entities to the model\n",
    "    for _, annotation in resume_train_data:\n",
    "        for ent in annotation['entities']:\n",
    "            ner.add_label(ent[2])\n",
    "            \n",
    "    # spacy model have few other pipelines too. As of now, we are ignoring those and focus only on NER  \n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "    \n",
    "    with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "        optimizer = nlp.begin_training()\n",
    "        for itn in range(10):\n",
    "            print('starting iteration '+ str(itn))\n",
    "            random.shuffle(resume_train_data) # shuffling the train data\n",
    "            losses = {}\n",
    "            index = 0\n",
    "            for text, annotations in resume_train_data:\n",
    "                try: \n",
    "                    # create Example\n",
    "                    doc = nlp.make_doc(text)\n",
    "                    example = Example.from_dict(doc, annotations)\n",
    "                    # Update the model with the text and its entities\n",
    "                    nlp.update([example], losses=losses, drop=0.3)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    pass\n",
    "                \n",
    "            print(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_entity_spans(data: list) -> list:\n",
    "    '''\n",
    "    Removes leading and trailing white spaces from entity spans and return the cleaned data.\n",
    "    '''\n",
    "    invalid_span_tokens = re.compile(r'\\s')\n",
    "\n",
    "    cleaned_data = []\n",
    "    for text, annotations in data:\n",
    "        entities = annotations['entities']\n",
    "        valid_entities = []\n",
    "        for start, end, label in entities:\n",
    "            valid_start = start\n",
    "            valid_end = end\n",
    "            while valid_start < len(text) and invalid_span_tokens.match(\n",
    "                    text[valid_start]):\n",
    "                valid_start += 1\n",
    "            while valid_end > 1 and invalid_span_tokens.match(\n",
    "                    text[valid_end - 1]):\n",
    "                valid_end -= 1\n",
    "            valid_entities.append([valid_start, valid_end, label])\n",
    "        cleaned_data.append([text, {'entities': valid_entities}])\n",
    "\n",
    "    return cleaned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trim the input data\n",
    "resume_train_data_cleaned = trim_entity_spans(resume_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build and train a spacy model \n",
    "train_model(resume_train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above training takes some time and that is why after the training, I am storing this model to the disk for future easy retrieval."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store this model to the disk\n",
    "nlp.to_disk('nlp_model_with_100_resumes_tuned')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
